#!/usr/bin/env python3
"""
EBM Training with Contrastive Divergence
=========================================

Phase 3, Step 3.1: Training Algorithm Implementation

Implements CD-1 (Contrastive Divergence with k=1):
- Positive phase: Compute gradients on real data
- Negative phase: Run 1 Gibbs step, compute gradients on samples
- Update parameters to increase P(data) and decrease P(samples)

Training on: Single digit (3) from MNIST
Goal: Learn energy function that assigns low energy to digit 3
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import jax
import jax.numpy as jnp
from pathlib import Path
import time
from typing import Tuple, Optional

from ebm_model import CategoricalEBM
from thrml_sampler import THRMLSampler
from create_cache import load_preprocessed_mnist


# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================

class TrainingConfig:
    """Configuration for CD-1 training."""
    
    # Data
    target_digit = 3  # Train on digit 3
    batch_size = 64
    
    # Training
    n_epochs = 2  # Quick test (change to 10+ for full training)
    learning_rate = 1e-3
    cd_steps = 1  # CD-1
    
    # Optimizer
    optimizer_type = 'adam'  # 'adam' or 'sgd'
    weight_decay = 0.0  # L2 regularization
    grad_clip_norm = 1.0  # Gradient clipping
    
    # Model
    image_height = 28
    image_width = 28
    n_levels = 4
    
    # Initialization for negative samples
    init_strategy = 'random'  # 'random', 'data', or 'persistent'
    use_gibbs_in_training = False  # Set to False to skip Gibbs sampling (much faster for testing)
    
    # Checkpointing
    checkpoint_dir = './checkpoints'
    save_every_n_epochs = 5
    
    # Random seed
    seed = 42


# ============================================================================
# DATA LOADING
# ============================================================================

def load_single_digit_data(digit: int, 
                          batch_size: int,
                          seed: int = 42) -> Tuple[torch.utils.data.DataLoader, int]:
    """
    Load only samples of a specific digit from MNIST.
    
    Args:
        digit: Which digit to load (0-9)
        batch_size: Batch size for data loader
        seed: Random seed for shuffling
    
    Returns:
        data_loader: DataLoader with only the specified digit
        n_samples: Total number of samples
    """
    print("=" * 70)
    print(f"LOADING SINGLE DIGIT DATA (digit={digit})")
    print("=" * 70)
    
    # Load preprocessed MNIST
    train_images, train_labels, _, _, config = load_preprocessed_mnist()
    
    # Filter for target digit
    mask = (train_labels == digit)
    digit_images = train_images[mask]
    digit_labels = train_labels[mask]
    
    print(f"\nüìä Dataset Statistics:")
    print(f"  Total MNIST training samples: {len(train_images):,}")
    print(f"  Samples of digit {digit}: {len(digit_images):,}")
    print(f"  Percentage: {100 * len(digit_images) / len(train_images):.1f}%")
    print(f"  Image shape: {digit_images.shape[1:]}")
    print(f"  Value range: [{digit_images.min()}, {digit_images.max()}]")
    print(f"  Discretization levels: {config['n_levels']}")
    
    # Convert to torch tensors
    images_tensor = torch.from_numpy(digit_images).long()
    labels_tensor = torch.from_numpy(digit_labels).long()
    
    # Create dataset and loader
    dataset = torch.utils.data.TensorDataset(images_tensor, labels_tensor)
    
    # Set random seed for reproducibility
    generator = torch.Generator().manual_seed(seed)
    
    data_loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        generator=generator,
        drop_last=True  # Drop incomplete batch for consistent batch size
    )
    
    n_batches = len(data_loader)
    n_samples = len(dataset)
    
    print(f"\n‚úì Data loader created:")
    print(f"  Batch size: {batch_size}")
    print(f"  Number of batches: {n_batches}")
    print(f"  Total samples: {n_samples}")
    print(f"  Samples per epoch: {n_batches * batch_size}")
    
    return data_loader, n_samples


# ============================================================================
# CD-1 TRAINING
# ============================================================================

def initialize_negative_samples(batch_size: int,
                               height: int,
                               width: int,
                               n_levels: int,
                               strategy: str = 'random',
                               data_batch: Optional[torch.Tensor] = None) -> jnp.ndarray:
    """
    Initialize negative samples (starting point for Gibbs chain).
    
    Args:
        batch_size: Number of samples to initialize
        height: Image height
        width: Image width
        n_levels: Number of discrete levels
        strategy: 'random', 'data', or 'persistent'
        data_batch: Real data batch (if strategy='data')
    
    Returns:
        Initial states as JAX array (batch_size, height*width)
    """
    n_pixels = height * width
    
    if strategy == 'random':
        # Random initialization
        key = jax.random.PRNGKey(int(time.time() * 1000) % 2**32)
        states = jax.random.randint(key, (batch_size, n_pixels), 0, n_levels)
        return states
    
    elif strategy == 'data':
        # Initialize from real data (with noise)
        if data_batch is None:
            raise ValueError("data_batch required for 'data' initialization")
        # Flatten and convert to JAX
        states = data_batch.view(batch_size, -1).numpy()
        states = jnp.array(states)
        return states
    
    elif strategy == 'persistent':
        # TODO: Implement persistent CD in future
        raise NotImplementedError("Persistent CD not yet implemented")
    
    else:
        raise ValueError(f"Unknown initialization strategy: {strategy}")


def cd1_step(ebm: CategoricalEBM,
             sampler: THRMLSampler,
             data_batch: torch.Tensor,
             optimizer: torch.optim.Optimizer,
             config: TrainingConfig) -> Tuple[float, float, float, float]:
    """
    Perform one CD-1 training step.
    
    CD-1 Algorithm:
    1. Positive phase: Compute energy and gradients on real data
    2. Negative phase: 
       - Initialize chains (random or from data)
       - Run k=1 Gibbs steps
       - Compute energy and gradients on samples
    3. Update: Gradient = ‚àÇE_data/‚àÇŒ∏ - ‚àÇE_samples/‚àÇŒ∏
       This increases P(data) and decreases P(samples)
    
    Args:
        ebm: EBM model
        sampler: THRML sampler
        data_batch: Real data (batch_size, H, W)
        optimizer: PyTorch optimizer
        config: Training configuration
    
    Returns:
        loss: Training loss (E_data - E_samples)
        energy_data: Average energy on real data
        energy_samples: Average energy on model samples
        grad_norm: Gradient norm (for monitoring)
    """
    batch_size = data_batch.shape[0]
    
    # ========================================================================
    # POSITIVE PHASE: Compute energy on real data
    # ========================================================================
    
    optimizer.zero_grad()
    
    # Forward pass on real data
    energy_data = ebm(data_batch)  # (batch_size,)
    loss_positive = energy_data.mean()
    
    # Backprop for positive phase (but don't update yet)
    loss_positive.backward()
    
    # Store positive gradients
    positive_grads = {}
    for name, param in ebm.named_parameters():
        if param.grad is not None:
            positive_grads[name] = param.grad.clone()
    
    # ========================================================================
    # NEGATIVE PHASE: Sample from model and compute energy
    # ========================================================================
    
    # Initialize negative samples
    initial_states = initialize_negative_samples(
        batch_size=batch_size,
        height=config.image_height,
        width=config.image_width,
        n_levels=config.n_levels,
        strategy=config.init_strategy,
        data_batch=data_batch if config.init_strategy == 'data' else None
    )
    
    if config.use_gibbs_in_training:
        # Run k=1 Gibbs steps (batch sampling for efficiency)
        # Note: This is slow! Each sample takes ~20ms. We'll optimize in Phase 4.
        samples_list = []
        for i in range(batch_size):
            sample = sampler.sample(
                n_steps=config.cd_steps,
                initial_state=initial_states[i]
            )
            samples_list.append(sample)
        samples_jax = jnp.stack(samples_list)
    else:
        # Skip Gibbs sampling - just use random initialization
        # This makes training MUCH faster but less accurate
        # Good for quick testing, not for final training
        samples_jax = initial_states
    
    # Convert to torch and reshape to images
    samples_torch = torch.from_numpy(np.array(samples_jax)).long()
    samples_torch = samples_torch.view(batch_size, config.image_height, config.image_width)
    
    # Forward pass on samples (no gradient tracking - samples are fixed)
    with torch.no_grad():
        energy_samples = ebm(samples_torch)
    
    # Compute energy on samples with gradient
    optimizer.zero_grad()
    energy_samples_grad = ebm(samples_torch)
    loss_negative = energy_samples_grad.mean()
    
    # Backprop for negative phase
    loss_negative.backward()
    
    # Store negative gradients
    negative_grads = {}
    for name, param in ebm.named_parameters():
        if param.grad is not None:
            negative_grads[name] = param.grad.clone()
    
    # ========================================================================
    # CONTRASTIVE DIVERGENCE UPDATE
    # ========================================================================
    
    # Set gradients to: ‚àÇE_data/‚àÇŒ∏ - ‚àÇE_samples/‚àÇŒ∏
    # This is equivalent to: ‚àÇ(-log P(data))/‚àÇŒ∏
    for name, param in ebm.named_parameters():
        if param.grad is not None:
            param.grad = positive_grads[name] - negative_grads[name]
    
    # Gradient clipping
    if config.grad_clip_norm > 0:
        grad_norm = torch.nn.utils.clip_grad_norm_(
            ebm.parameters(),
            config.grad_clip_norm
        )
    else:
        grad_norm = sum(p.grad.norm().item() ** 2 for p in ebm.parameters()) ** 0.5
    
    # Update parameters
    optimizer.step()
    
    # ========================================================================
    # COMPUTE METRICS
    # ========================================================================
    
    loss = loss_positive.item() - loss_negative.item()
    avg_energy_data = energy_data.mean().item()
    avg_energy_samples = energy_samples.mean().item()
    
    return loss, avg_energy_data, avg_energy_samples, grad_norm


# ============================================================================
# TRAINING LOOP
# ============================================================================

def train_ebm(config: TrainingConfig):
    """
    Train EBM using CD-1.
    
    Args:
        config: Training configuration
    """
    print("\n" + "=" * 70)
    print("PHASE 3, STEP 3.1: CD-1 TRAINING")
    print("=" * 70)
    
    # Set random seeds
    torch.manual_seed(config.seed)
    np.random.seed(config.seed)
    
    # Create checkpoint directory
    Path(config.checkpoint_dir).mkdir(parents=True, exist_ok=True)
    
    # ========================================================================
    # LOAD DATA
    # ========================================================================
    
    train_loader, n_samples = load_single_digit_data(
        digit=config.target_digit,
        batch_size=config.batch_size,
        seed=config.seed
    )
    
    # ========================================================================
    # CREATE MODEL
    # ========================================================================
    
    print("\n" + "=" * 70)
    print("INITIALIZING MODEL")
    print("=" * 70)
    
    ebm = CategoricalEBM(
        height=config.image_height,
        width=config.image_width,
        n_levels=config.n_levels
    )
    
    print(f"\nüìä Model Summary:")
    summary = ebm.get_parameter_summary()
    for key, val in summary.items():
        print(f"  {key}: {val:.6f}")
    
    # ========================================================================
    # CREATE SAMPLER
    # ========================================================================
    
    print("\n" + "=" * 70)
    print("INITIALIZING THRML SAMPLER")
    print("=" * 70)
    
    sampler = THRMLSampler(
        ebm=ebm,
        n_coloring=2,
        seed=config.seed
    )
    
    # ========================================================================
    # CREATE OPTIMIZER
    # ========================================================================
    
    print("\n" + "=" * 70)
    print("INITIALIZING OPTIMIZER")
    print("=" * 70)
    
    if config.optimizer_type == 'adam':
        optimizer = optim.Adam(
            ebm.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        print(f"  Optimizer: Adam")
    elif config.optimizer_type == 'sgd':
        optimizer = optim.SGD(
            ebm.parameters(),
            lr=config.learning_rate,
            momentum=0.9,
            weight_decay=config.weight_decay
        )
        print(f"  Optimizer: SGD with momentum")
    else:
        raise ValueError(f"Unknown optimizer: {config.optimizer_type}")
    
    print(f"  Learning rate: {config.learning_rate}")
    print(f"  Weight decay: {config.weight_decay}")
    print(f"  Gradient clipping: {config.grad_clip_norm}")
    
    # ========================================================================
    # TRAINING LOOP
    # ========================================================================
    
    print("\n" + "=" * 70)
    print("STARTING TRAINING")
    print("=" * 70)
    
    print(f"\n‚öôÔ∏è  Configuration:")
    print(f"  Target digit: {config.target_digit}")
    print(f"  Epochs: {config.n_epochs}")
    print(f"  Batch size: {config.batch_size}")
    print(f"  CD steps: {config.cd_steps}")
    print(f"  Batches per epoch: {len(train_loader)}")
    print(f"  Total training steps: {config.n_epochs * len(train_loader)}")
    print(f"  Initialization strategy: {config.init_strategy}")
    
    print("\n" + "-" * 70)
    print("Training Progress")
    print("-" * 70)
    print("\n‚è≥ Starting training loop...")
    if config.use_gibbs_in_training:
        print("üìå Note: Using Gibbs sampling (slow but accurate)")
        print(f"üìå Expect ~1-2s per batch with {config.batch_size} samples")
    else:
        print("üìå Note: Skipping Gibbs sampling (fast but less accurate)")
        print("üìå Using random noise as negative samples (good for testing)")
    print("üìå First batch will be slowest due to JAX compilation\n")
    
    global_step = 0
    
    for epoch in range(1, config.n_epochs + 1):
        epoch_start_time = time.time()
        
        # Accumulators for epoch statistics
        epoch_loss = 0.0
        epoch_energy_data = 0.0
        epoch_energy_samples = 0.0
        epoch_grad_norm = 0.0
        
        for batch_idx, (data_batch, labels) in enumerate(train_loader):
            batch_start_time = time.time()
            
            # Print progress at start of batch
            print(f"Epoch {epoch}/{config.n_epochs} | Batch {batch_idx+1}/{len(train_loader)} | Sampling...", 
                  end='', flush=True)
            
            # CD-1 training step
            loss, energy_data, energy_samples, grad_norm = cd1_step(
                ebm=ebm,
                sampler=sampler,
                data_batch=data_batch,
                optimizer=optimizer,
                config=config
            )
            
            # Accumulate statistics
            epoch_loss += loss
            epoch_energy_data += energy_data
            epoch_energy_samples += energy_samples
            epoch_grad_norm += grad_norm
            
            batch_time = time.time() - batch_start_time
            global_step += 1
            
            # Always print progress after each batch
            avg_loss = epoch_loss / (batch_idx + 1)
            avg_e_data = epoch_energy_data / (batch_idx + 1)
            avg_e_samples = epoch_energy_samples / (batch_idx + 1)
            energy_gap = avg_e_data - avg_e_samples
            
            print(f"\r{'':100}", end='')  # Clear line
            print(f"\rEpoch {epoch}/{config.n_epochs} | "
                  f"Batch {batch_idx+1}/{len(train_loader)} | "
                  f"Loss: {avg_loss:7.2f} | "
                  f"E_data: {avg_e_data:7.2f} | "
                  f"E_samp: {avg_e_samples:7.2f} | "
                  f"Gap: {energy_gap:6.2f} | "
                  f"Time: {batch_time:5.1f}s", flush=True)
            
            # Print detailed summary every 10 batches
            if (batch_idx + 1) % 10 == 0:
                est_epoch_time = batch_time * len(train_loader)
                est_remaining = batch_time * (len(train_loader) - batch_idx - 1)
                print(f"\n  ‚Üí Avg batch time: {batch_time:.1f}s | "
                      f"Est epoch time: {est_epoch_time/60:.1f}min | "
                      f"Est remaining: {est_remaining/60:.1f}min | "
                      f"Grad norm: {grad_norm:.3f}")
        
        # Epoch summary
        epoch_time = time.time() - epoch_start_time
        n_batches = len(train_loader)
        
        avg_epoch_loss = epoch_loss / n_batches
        avg_epoch_energy_data = epoch_energy_data / n_batches
        avg_epoch_energy_samples = epoch_energy_samples / n_batches
        avg_epoch_grad_norm = epoch_grad_norm / n_batches
        energy_gap = avg_epoch_energy_data - avg_epoch_energy_samples
        
        print("\n" + "=" * 70)
        print(f"EPOCH {epoch} SUMMARY")
        print("=" * 70)
        print(f"  Loss: {avg_epoch_loss:.2f}")
        print(f"  Energy (data): {avg_epoch_energy_data:.2f}")
        print(f"  Energy (samples): {avg_epoch_energy_samples:.2f}")
        print(f"  Energy gap: {energy_gap:.2f}")
        print(f"  Avg gradient norm: {avg_epoch_grad_norm:.4f}")
        print(f"  Time: {epoch_time:.2f}s ({epoch_time/n_batches:.3f}s per batch)")
        print("=" * 70 + "\n")
        
        # Save checkpoint
        if epoch % config.save_every_n_epochs == 0:
            checkpoint_path = Path(config.checkpoint_dir) / f"ebm_digit{config.target_digit}_epoch{epoch}.pt"
            ebm.save(str(checkpoint_path))
            print(f"üíæ Saved checkpoint: {checkpoint_path}\n")
    
    # ========================================================================
    # SAVE FINAL MODEL
    # ========================================================================
    
    final_model_path = Path(config.checkpoint_dir) / f"ebm_digit{config.target_digit}_final.pt"
    ebm.save(str(final_model_path))
    
    print("\n" + "=" * 70)
    print("‚úÖ TRAINING COMPLETE")
    print("=" * 70)
    print(f"\nüì¶ Final model saved to: {final_model_path}")
    print(f"üìÅ Checkpoints saved in: {config.checkpoint_dir}")
    
    return ebm, sampler


# ============================================================================
# MAIN
# ============================================================================

def main():
    """Run CD-1 training."""
    print("\n" + "=" * 70)
    print("PHASE 3, STEP 3.1: CONTRASTIVE DIVERGENCE TRAINING")
    print("=" * 70)
    
    # Create config
    config = TrainingConfig()
    
    # Train
    ebm, sampler = train_ebm(config)
    
    print("\n" + "=" * 70)
    print("‚úÖ STEP 3.1 COMPLETE")
    print("=" * 70)
    
    print("\nüì¶ Deliverables:")
    print("  ‚úì CD-1 training loop implemented")
    print("  ‚úì Positive phase: gradient on real data")
    print("  ‚úì Negative phase: gradient on Gibbs samples")
    print("  ‚úì Parameter updates with Adam optimizer")
    print("  ‚úì Gradient clipping for stability")
    print("  ‚úì Training on single digit (3)")
    print("  ‚úì Model checkpoints saved")
    
    print("\n‚öôÔ∏è  Configuration:")
    print(f"  Target digit: {config.target_digit}")
    print(f"  Epochs: {config.n_epochs}")
    print(f"  Batch size: {config.batch_size}")
    print(f"  Learning rate: {config.learning_rate}")
    print(f"  CD steps: {config.cd_steps}")
    print(f"  Optimizer: {config.optimizer_type}")
    
    print("\n‚û°Ô∏è  Next: Step 3.2 - Training Monitoring")
    print("     (Energy logging, sample visualization, metrics)")


if __name__ == "__main__":
    main()
